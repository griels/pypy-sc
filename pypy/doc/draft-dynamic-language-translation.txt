============================================================
       Compiling dynamic language implementations
============================================================


The analysis of dynamic languages
===============================================

Dynamic languages are definitely not new on the computing scene.  
However, new conditions like increased computing power and designs driven
by larger communities have enabled the emergence of new aspects in the
recent members of the family, or at least made them more practical than
they previously were.  The following aspects in particular are typical not
only of Python but of most modern dynamic languages:

* The driving force is not minimalistic elegance.  It is a balance between
  elegance and practicality, and rather un-minimalistic -- the feature
  sets built into languages tend to be relatively large and growing
  (to some extent, depending on the language).

* High abstractions and theoretically powerful low-level primitives are
  generally ruled out in favor of a larger number of features that try to
  cover the most common use cases.  In this respect, one could even regard
  these languages as mere libraries on top of some simpler (unspecified)
  language.

* Implementation-wise, language design is no longer driven by a desire to
  enable high performance; any feature straightforward enough to achieve
  with an interpreter is candidate.  As a result, compilation and most
  kinds of static inference are made impossible due to this dynamism
  (unless they are simply tedious due to the size of the language).


No Declarations
--------------------------

The notion of "declaration", central in compiled languages, is entirely
missing in Python.  There is no aspect of a program that must be declared;
the complete program is built and run by executing statements.  Some of
these statements have a declarative look and feel; for example, some
appear to be function or class declarations.  Actually, they are merely
statements that, when executed, build a function or class object.  A
reference to the new object is then stored in a namespace from where it
can be accessed.  Units of programs -- modules, whose source is one
file each -- are similarily mere objects in memory, built on demand by some
other module executing an ``import`` statement.  Any such statement --
class construction or module import -- can be executed at any time during
the execution of a program.

This point of view should help explain why an analysis of a program is
theoretically impossible: there is no declared structure.  The program
could for example build a class in completely different ways based on the
results of NP-complete computations or external factors.  This is not just
a theoretical possibility but a regularly used feature: for example, the
pure Python module ``os.py`` provides some OS-independent interface to
OS-specific system calls, by importing internal OS-specific modules and
completing it with substitute functions, as needed by the OS on which
``os.py`` turns out to be executed.  Many large Python projects use custom
import mechanisms to control exactly how and from where each module is
loaded, by tampering with import hooks or just emulating parts of the
``import`` statement manually.

In addition, there are of course classical (and only partially true)
arguments against compiling dynamic languages (there is an ``eval``
function that can execute arbitrary code, and introspection can change
anything at run-time), but we consider the argument outlined above as more
fundamental to the nature of dynamic languages.


The analysis of live programs
-----------------------------------

How can we perform some static analysis on a program written in a dynamic
language while keeping to the spirit of `No Declarations`_, i.e. without
imposing that the program be written in a static way in which these
declarative-looking statements would actually *be* declarations?

The approach of PyPy is, first of all, to perform analysis on live
programs in memory instead of dead source files.  This means that the
program to analyse is first fully imported and initialized, and once it
has reached a state that is deemed advanced enough, we limit the amount of
dynamism that is allowed *after this point* and we analyse the program's
objects in memory.  In some sense, we use the full Python as a
preprocessor for a subset of the language, called RPython.  Informally,
RPython is Python without the operations and effects that are not supported
by our analysis toolchain (e.g. class creation, and most non-local effects).

Of course, putting more efforts into the toolchain would allow us to
support a larger subset of Python.  We do not claim that our toolchain --
which we describe in the sequel of this paper -- is particularly advanced.
To make our point, let us assume a given an analysis tool, which supports
a given subset of a language.  Then:

* Analysing dead source files is equivalent to giving up all dynamism
  (as far as unsupported by this tool).  This is natural in the presence of
  static declarations.

* Analysing a frozen memory image of a program that we loaded and
  initialized is equivalent to giving up all dynamic after a certain point
  in time.  This is natural in image-oriented environments like Smalltalk,
  where the program resides in memory and not in files in the first place.

Our approach goes further and analyses *live* programs in memory:
the program is allowed to contain fully dynamic sections, as long as these
sections are entered a *bounded* number of times.
For example, the source code of the PyPy
interpreter, which is itself written in this bounded-dynamism style, makes
extensive use of the fact that it is possible to build new classes at any
point in time -- not just during an initialization phase -- as long as this
number of bounded.  E.g. `interpreter/gateway.py`_ builds a custom class
for each function that some variable can point to.  There is a finite
number of functions in total, so this can obviously only create
a finite number of extra classes.  But the precise set of functions that
need a corresponding class is difficult to manually compute in advance;
instead, the code that builds and cache a new class is invoked by the
analysis tool itself each time it discovers that a new function object can
reach the corresponding point.

This approach is derived from dynamic analysis techniques that can support
unrestricted dynamic languages by falling back to a regular interpreter for
unsupported features (e.g. Psyco, described in
http://psyco.sourceforge.net/psyco-pepm-a.ps.gz).
The above argumentation should have shown why we think that being similarily
able to fall back to regular interpretation for parts that cannot be
understood is a central feature of the analysis of dynamic languages.


Concrete and abstract interpretation
======================================================

Object Spaces
---------------------------------

The semantics of Python can be roughly divided in two groups: the syntax of
the language, which focuses on control flow aspects, and the object semantics,
which define how various types of objects react to various operations and
methods.  As it is common in all languages of the family, both the
syntactic elements and the object semantics are complex and at times
complicated (as opposed to more classical languages that tend to subsume
one aspect to the other: for example, Lisp's execution semantics are almost
trivial).

This observation led us to the concept of *Object Space*.  An interpreter can
be divided in two non-trivial parts: one for handling compilation to and
interpretation of pseudo-code (control flow aspects) and one implementing
the object library's semantics.  The former, called *bytecode interpreter*,
considers objects as black boxes; any operation on objects requested by the
bytecode is handled over to the object library, called *object space*.
The point of this architecture is, precisely, that neither of these two
components is trivial; separating them explicitely, with a well-defined
interface inbetween, allows each part to be reused independently.  This is
a major flexibility feature of PyPy: we can for example insert proxy object
spaces in front of the real one, like the `Thunk Object Space`_ adding lazy
evaluation of objects.

Note that the term "object space" has already been reused for other
dynamic language implementations, e.g. XXX for Perl 6.


Abstract interpretation
------------------------------

In the sequel of this paper, we will consider another application
of the object space separation.  The analysis we perform in PyPy
is whole-program type inference.  The analysis of the non-dynamic
parts themselves is based on their `abstract interpretation`_.
PyPy has an alternate object space called the `Flow Object Space`_,
whose objects are empty placeholders.  The over-simplified view
is that to analyse a function, we bind its input arguments to such
placeholders, and execute the function -- i.e. let the interpreter follow
its bytecode and invoke the object space for each operations, one by one.  
The Flow object space records each operation when it is issued, and
returns a new placeholder as a result.  At the end, the list of recorded
operations, along with the involved placeholders, gives an assembler-like
view of what the function performs.

The global picture is then to run the program while switching between the
flow object space for static enough functions, and a standard, concrete
object space for functions or initializations requiring the full dynamism.

If the placeholders are endowed with a bit more information, e.g. if they
carry a type information that is propagated to resulting placeholders by
individual operations, then our abstract interpretation simultaneously
performs type inference.  This is, in essence, executing the program while
abstracting out some concrete values and replacing them with the set of
all values that could actually be there.  If the sets are broad enough,
then after some time we will have seen all potential value sets along each
possible code paths, and our program analysis is complete.

An object space is thus an *interpretation domain*; the Flow Object Space
is an *abstract interpretation domain*.  We are thus interpreting the
program while switching dynamically between several abstraction levels.
This is possible because our design allows the *same* interpreter to work
with a concrete or an abstract object space.

Following parts of the program at the abstract level allows us to deduce
general information about the program, and for parts that cannot be analysed
we switch to the concrete level.  The restrictions placed on the program
to statically analyse are that to be crafted in such a way that this process
eventually terminates; from this point of view, more abstract is better (it
covers whole sets of objects in a single pass).  Thus the compromize that
the author of the program to analyse faces are less strong but more subtle
than not using a specific set of dynamic features at all, but using them
sparsingly enough.


The PyPy analysis toolchain
===========================================

The previous sections have developed a theoretical point of view that
differs significantly from what we have implemented, for many reasons.
The devil is in the details.


Flow Object Space
---------------------------------

In our bytecode-interpreter design evaluation responsibilities are
split between the Object Space, frames and the so-called execution
context. The latter two object kinds are properly part of the
interpretation engine, while the object space implements all
operations on values which are treated as black boxes by the engine.

The Object Space plays the role of a factory for execution contexts,
whose base implementation is supplied by the engine, and exposes hooks
triggered when frames are entered, left and before each bytecode,
allowing to gather a trace of the execution.

Frames have run/resume methods which embed the interpretation loop,
These methods take an execution context invoking the appropriate hooks
at the corresponding situations.

The Flow Object Space in our current design is responsible of
constructing a flow graph for a single function using abstract
interpretation.  The domain on which the Flow Space operates comprises
variables and constant objects. They are stored as such in the frame
objects without problems because by design the interpreter engine treat
them as black boxes.


Construction of flow graphs
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Concretely, the Flow Space plugs itself in the interpreter as an object
space and supplies a derived execution context implementation.  It also
wraps a fix-point loop around invocations of the frame resume method.
In our current design, this fix-point searching is implemented by
interrupting the normal interpreter loop in the frame after every
bytecode, and comparing the state with previously-seen states.  These
states describe the execution state for the frame at a given point.
They are synthesised out of the frame by the Flow Space; they contain
position-dependent data (current bytecode index, current exception
handlers stack) as well as a flattened list of all variables and
constants currently handled by the frame.

The Flow Space constructs the flow graph, operation after operation, as
a side effect of seeing these operations performed by the interpretation
of the bytecode.  During construction, blocks in the graph all have an
associated frame state. The Flow Space start from an empty block with an
a frame state corresponding to a frame freshly initialized, with a new
variables for each input argument of the analysed function.  It proceeds
by recording the operations in this block, as follows: when an operation
is delegated to the Flow Space by the frame interpretation loop, either
a constant result is produced -- in the case of constant arguments to an
operation with no side-effects -- or a fresh new variable is produced.
In the latter case, the operation (together with its input variables and
constant arguments, and its output variable) is recorded in the current
block and the new variable is returned as result to the frame
interpretation loop.

When a new bytecode is about to be executed, as signalled by the
bytecode hook, the Flow Space considers the frame state corresponding to
the current frame contents.  This state is compared with the existing
states attached to the blocks produced so far.  If the state was not
seen before, the Flow Space creates a new block in the graph.  If the
same state was already seen before, then a backlink to the previous
block is inserted, and the abstract interpretation stops here.  If only
a "similar enough" state was seen so far, then the current and the
previous states are merged to produce a more general state.

In more details, "similar enough" is defined as having the same
position-dependant part, the so-called "non-mergeable frame state",
which mostly means that only frame states corresponding to the same
bytecode position can ever be merged.  This process thus produces blocks
that are generally in one-to-one correspondance with the bytecode
positions seen so far.  The exception to this rule is in the rare cases
where frames from the same bytecode position have a different
non-mergeable state, which typically occurs during the "finally" part of
a "try: finally:" construct, where the details of the exception handler
stack differs according to whether the "finally" part was entered
normally or as a result of an exception.

If two states have the same non-mergeable part, they can be merged using
a "union" operation: only two equal constants unify to a constant of the
same value; all other combinations (variable-variable or
variable-constant) unify to a fresh new variable.

In summary, if some previously associated frame state for the next
byecode can be unified with the current state, then a backlink to the
corresponding existing block is inserted; additionally, if the unified
state is strictly more general than the existing one, then the existing
block is cleared, and we proceed with the generalized state, reusing the
block.  (Reusing the block avoids the proliferation of over-specific
blocks.  Ror example, without this, all loops would typically have their
first pass unrolled with the first value of the counter as a constant;
instead, the second pass through the loop that the Flow Space does with
the counter generalized as a variable will reuse the same entry point
block, and any further blocks from the first pass are simply
garbage-collected.)


Branching
~~~~~~~~~

Branching on conditions by the engine usually involves querying the
truth value of a object through the ``is_true`` space operation.  When
this object is a variable, the result is not statically known; this
needs special treatment to be able to capture both possible flow paths.
In theory, this would require continuation support at the language level
so that we can pretend that ``is_true`` returns twice into the engine,
once for each possible answer, so that the Flow Space can record both
outcomes.  Without proper continuations in Python, we have implemented a
more explicit scheme that we describe below.  (The approach is related
to the one used in Psyco_, where continuations would be entierely
inpractical, as described in the `ACM SIGPLAN 2004 paper`_.)

At any point in time, multiple pending blocks can be scheduled for
abstract interpretation by the Flow Space, which proceeds by picking one
of them and reconstructing a frame from the frame state associated with
the block.  This frame reconstruction is actually delegated to the
block, which also returns a so-called "recorder" through which the Flow
Space will append new space operations to the block.  The recorder is
also responsible for handling the ``is_true`` operation.

A normal recorder simply appends the space operations to the block from
which it comes from.  However, when it sees an ``is_true`` operation, it
creates and schedules two special blocks (one for the outcome ``True``
and one for the outcome ``False``) which don't have an associated frame
state.  The previous block is linked to the two new blocks with
conditional exits.  At this point, abstract interpretation stops (i.e.
an exception is raised to interrupt the engine).

The special blocks have no frame state, and cannot be used to setup a
frame: indeed, unlike normal blocks, which correspond to the state of
the engine between the execution of two bytecode, special blocks
correspond to a call to ``is_true`` issued the engine.  The details of
the engine state (internal call stack and local variables) are not
available at this point.

However, it is still possible to put the engine back into the state
where it was calling ``is_true``.  This is what occurs later on, when
one of the special block is scheduled for further execution: the block
considers its previous block, and possibly its previous block's previous
block, and so on up to the first normal block.  As we can see, these
blocks form a binary tree of special blocks with a normal block at the
root.  A special block thus corresponds to a branch in the tree, whose
path is described by a list of outcomes -- a list of boolean values.  We
can thus restore the state of any block by starting from the root and
asking the engine to replay the execution from there; intermediate
``is_true`` calls issued by the engine are answered according to the
list of outcomes until we reach the desired state.

This is implemented by having a special blocks (called ``EggBlocks``
internally, whereas normal blocks are ``SpamBlocks``) return a chain of
recorders: one so-called "replaying" recorder for each of the parent
blocks in the tree, followed by a normal recorder for the block itself.
When the engine replays the execution from the root of the tree, the
intermediate recorders check (for consistency) that the same operations
as the ones already recorded are issued again, ending in a call to
``is_true``; at this point, the replaying recorder gives the answer
corresponding to the branch to follow, and switch to the next recorder
in the chain.

This mechanism ensures that all flow paths are considered, including
different flow paths inside the engine and not only flow paths that are
explicit in the bytecode.  For example, an ``UNPACK_SEQUENCE`` bytecode
in the engine iterates over a sequence object and checks that it
produces exactly the expected number of values; so the single bytecode
``UNPACK_SEQUENCE n`` generates a tree with ``n+1`` branches
corresponding to the ``n+1`` times the engine asks the iterator if it
has more elements to produce.  A simpler example is a conditional jump,
which will generate a pair of special blocks for the ``is_true``, each
of which consisting only in a jump to the normal block corresponding to
the next bytecode -- either the one following the conditional jump, or
the target of the jump, depending on whether the replayer answered
``False`` or ``True`` to the ``is_true``.

Note a limitation of this mechanism: the engine cannot use an unbounded
loop to implement a single bytecode.  All *loops* must still be
explicitly present in the bytecodes.  The reason is that the Flow Space
can only insert backlinks between bytecodes.


Dynamic merging
~~~~~~~~~~~~~~~

For simplicity, we have so far omitted a point in the description of how
frame states are associated to blocks.  In our implementation, there is
not necessarily a block corresponding to each bytecode position (or more
precisely each non-mergeable state): we avoid creating blocks at all if
they would stay empty.  This is done by tentatively running the engine
on a given frame state and seeing if it creates at least one operation;
if it does not, then we simply continue with the new frame state without
having created a block for the previous frame state.  The previous frame
state is discarded without having even tried to compare it with
already-seen state to see if it merges.

The effect of this is that merging only occurs at the beginning of a
bytecode that actually produces an operation.  This allows some amount
of constant-folding: for example, the two functions below produce the
same flow graph::

    def f(n):             def g(n):
        if n < 0:             if n < 0:
            n = 0                 return 1
        return n+1            else:
                                  return n+1

because the two branches of the condition are not merged where the
``if`` statement syntactically ends: the ``True`` branch propagates a
constant zero in the local variable ``n``, and the following addition is
constant-folded and does not generate a residual operation.

Note that this feature means that the Flow Space is not guaranteed to
terminate.  The analysed function can contain arbitrary computations on
constant values (with loops) that will be entierely constant-folded by
the Flow Space.  A function with an obvious infinite loop will send the
Flow Space following the loop ad infinitum.  This means that it is
difficult to give precise conditions for when the Flow Space terminates
and which complexity it has.  Informally, "reasonable" functions should
not create problems: it is uncommon for a function to perform
non-trivial constant computations at run-time; and the complexity of the
Flow Space can more or less be bound by the run-time complexity of the
constant parts of the function itself, if we ignore pathological cases
where a part of a function contains infinite loops but cannot be entered
at run-time for some reasons unknown to the Flow Space.


Geninterp
~~~~~~~~~

Introducing `Dynamic merging`_ can be seen as a practical move: it does
not, in practice, prevent even large functions to be analysed reasonably
quickly, and it is useful to simplify the flow graphs of some functions.
This is specially true for functions that are themselves automatically
generated.

In the PyPy interpreter, for convenience, some of the core functionality
has been written as application-level Python code, which means that the
interpreter will consider some core operations as calls to further
application-level code.  This has, of course, a performance hit due to
the interpretation overhead.  To minimize this overhead, we
automatically turn some of this application-level code into
interpreter-level code, as follows.  Consider the following trivial
example function at application-level::

    def f_app(n):
        return n+1

Interpreting it, the engine just issues an ``add`` operation on the
object space, which means that it is mostly equivalent to the following
interpreter-level function::

    def f_interp(space, wrapped_n):
        return space.add(wrapped_n, wrapped_1)

The translation from ``f_app`` to ``f_interp`` can be done automatically
by using the Flow Space as well: we produce the flow graph of ``f_app``
using the techniques described above, and then we turn the resulting
flow graph into ``f_interp`` by generating for each operation a call to
the corresponding method of ``space``.

This process looses the original syntactic structure of ``f_app``,
though; the flow graph is merely a collection of blocks that jump to
each other.  It is not always easy to reconstruct the structure from the
graph (or even possible at all, in some cases where the flow graph does
not exactly follow the bytecode).  So, as is common for code generators,
we use a workaround to the absence of explicit gotos::

    def f_interp(...):
        next_block = 0
        while True:

            if next_block == 0:
                ...
                next_block = 1

            if next_block == 1:
                ...

This produces Python code that is particularly sub-efficient when it is
interpreted; however, if it is further re-analysed by the Flow Space,
dynamic merging will ensure that ``next_block`` will always be
constant-folded away, instead of having the various possible values of
``next_block`` be merged at the beginning of the loop.


Annotator 
---------------------------------

The annotator is the type inference part of our toolchain.  The
annotator infers *types* in the following sense: given a program
considered as a family of control flow graphs, it assigns to each
variable of each graph a so-called *annotation*, which describes what
are the possible run-time objects that this variable will contain.  Note
that in the literature such an annotation is usually called a type, but
we prefer to avoid this terminology to avoid confusion with the Python
notion of the concrete type of an object.  Annotations are sets of
possible values that is not always exactly the set of all objects of a
specific Python type.

We will first expose a simplified, static model of how the annotator
works, and then hint at some differences between the model and the
reality.


Static model
~~~~~~~~~~~~

The annotator can be considered as taking as input a finite family of
functions calling each other, and working mainly on the control flow
graphs of each of these functions as built by the `Flow Object Space`_.
Additionally, for a particular "entry point" function, each input
argument is given a user-specified annotation.

The goal of the annotator is to find the most precise annotation that
can be given to each variable of all control flow graphs while
respecting constrains imposed by the operations in which these variables
are involved.

More precisely, it is usually possible to deduce information about the
result variable of an operation given information about its arguments.
For example, we can say that the addition of two integers must be an
integer.  Most programming languages have this property.  However,
Python -- like many languages not specifically designed with type
inference in mind -- does not possess a type system that allows much
useful information to be derived about variables based on how they are
*used*; only on how they were *produced*.  For example, a number of very
different built-in types can be involved in an addition; the meaning of
the addition and the type of the result depends on the type of the input
arguments.  Merely knowing that a variable will be used in an addition
does not give much information per se.  For this reason, our annotator
works by flowing annotations forward, operation after operation, i.e. by
performing abstract interpretation of the flow graphs.  In a sense, it
is a more naive approach than the one taken by type systems specifically
designed to enable more advanced inference algorithms.  For example,
`Hindley-Milner`_ type inference works in an inside-out direction, by
starting from individual operations and propagating type constrains
outwards.

Naturally, simply propagating annotations forward requires the use of a
fixpoint algorithm in the presence of loops in the flow graphs or in the
inter-procedural call graph.  Indeed, we flow annotations forward from
the beginning of the entry point function into each block, operation
after operation, and follow all calls recursively.  During this process,
each variable along the way gets an annotation.  In various cases,
e.g. when we close a loop, the previously assigned annotations can be
found to be too restrictive.  In this case, we generalize them to allow
for a larger set of possible run-time values, and schedule the block
where they appear for reflowing.  The more general annotations can
generalize the annotations of the results of the variables in the block,
which in turn can generalize the annotations that flow into the
following blocks, and so on.  This process continues until a fixpoint is
reached.

We can consider that all variables are initially assigned the "bottom"
annotation corresponding to an empty set of possible run-time values.
Annotations can only ever be generalized, and the model is simple enough
to show that there is no infinite chain of generalization, so that this
process necessarily terminates, as we will show in the sequel.


Annotation model
~~~~~~~~~~~~~~~~

::

                ____________ Top ___________
               /      /       |       \     \
              /      /        |        \     \
             /      /         |         |     \
            /   NullableStr   |         |      |
          Int     /   \       |       (lists)  |
          /     Str    \  (instances)   |    (pbcs)
    NonNegInt     \     \      \        |      |
          \       Char   \      \      /      /     
          Bool      \     \      \    /      /
            \        \     `----- None -----'
             \        \           /
              \        \         /
               `--------`-- Bottom


                             Top
                              |
                              |
                              |
                       NuInst(object)
                          /      / \
                  Inst(object)  /   \
                     /      \  /     \
                    /        \/       \
                   /         /\        \
                  /         /  \        \
                 /         /    \        \
                /  NuInst(cls2)  \     NuInst(cls1)
               /   /      \       \     /  /
           Inst(cls2)      \  Inst(cls1)  / 
                 \          \    /       /
                  \          \  /       /
                   \          \/       /
                    \         /\      /
                     \       /   None
                      \     /  /
                        Bottom



             __________________ Top __________________
            /            /     /   \     \            \
           /            /     /     \     \            \
          /            /     /       \     \            \
    List(v_1)       ...        ...        ...         List(v_n)
          \            \     \       /     /            /
           \            \     \     /     /            /
            \            \     \   /     /            /
             '------------'--- None ----'------------'



    Bot

    Top
    
    Int
    
    NonNegInt

    Bool

    Str
    
    NullableStr
    
    Char
    
    Inst(class)
    
    List(x)
    
    Dict(x, y)

    Tup(ann_1, ..., ann_n)
    
    Pbc({... a finite set ...})
    
         with:   None
                 f
                 class
                 class.f
    
    v_n = op(v_n1, ...) | v_n', v_n''
    
    v_class.attr
    
    v_n: Annotation

    for each function f:
        arg_f_1 ... arg_f_n
        returnvar_f


    E: eq rel on V
    b: V->A
    V: set of variables
    A: fixed lattice of the above annotation terms



         E(x,y)
      ----------------------------------------
               merge_into(x,y)
               merge_into(y,x)


         z=add(x,y), b(x)=List(v), b(y)=List(w)
      --------------------------------------------
               E' = E union (v~w)
               b' = b with (z->List(v))


         z=add(x,y), b(x)<=NullableStr, b(y)<=NullableStr
      ------------------------------------------------------
               b' = b with (z->Str)


         merge_into(x,y), b(x)=List(v), b(y)=List(w)
      -------------------------------------------------
               E' = E union (v~w)


         merge_into(x,y), b(x) and b(y) not both Lists
      ---------------------------------------------------
               b' = b with (y->b(x)\/b(y))


         z=new_list() | z'
      -------------------------------------
               b' = b with (z->List(z'))


         z=getitem(x,y) | z', b(x)=List(v)
      --------------------------------------------
               E' = E union (z'~v)
               b' = b with (z->b(z'))


         setitem(x,y,z), b(x)=List(v)
      --------------------------------------------
               merge_into(z,v)


         z=getattr(x,attr) | z', b(x)=Inst(A)
      ---------------------------------------------------------------------
               E' = E union (A.attr ~ A'.attr)  for all A' subclass of A
               E' = E union (z' ~ A.attr)
               b' = b with (z->lookup_filter(b(z'), A))


         setattr(x,attr,z), b(x)=Inst(A)
      ---------------------------------------------------------------------
               assert b(z) is not a Pbc containing methods
               E' = E union (A.attr ~ A'.attr)  for all A' subclass of A
               merge_into(z, A.attr)


         z=simplecall(x,y1,...,yn), b(x)=Pbc(set)
      ---------------------------------------------------------------------
           for each c in set:
               if c is a function:
                   E' = E union (z~returnvar_c)
                   merge_into(y1, arg_c_1)
                   ...
                   merge_into(yn, arg_c_n)
               if c is a class:
                   let f = c.__init__
                   b' = b with (z->b(z)\/Inst(c))
                   b' = b with (arg_f_1->b(arg_f_1)\/Inst(c))
                   merge_into(y1, arg_f_2)
                   ...
                   merge_into(yn, arg_f_(n+1))
               if c is a method:
                   let class.f = c
                   E' = E union (z~returnvar_f)
                   b' = b with (arg_f_1->b(arg_f_1)\/Inst(class))
                   merge_into(y1, arg_f_2)
                   ...
                   merge_into(yn, arg_f_(n+1))


    lookup_filter(Pbc(set), class) = Pbc(newset) where
        we only keep in newset the non-methods, and the following methods:
         * the ones bound to a strict subclass of 'class', and
         * among the methods bound the 'class' or superclasses, only the
             one from the most derived class.
    lookup_filter(NonPbcAnnotation, class) = NonPbcAnnotation


XXX model and rules

XXX constant propagation

Prebuilt constants
~~~~~~~~~~~~~~~~~~

XXX

Mutable objects
~~~~~~~~~~~~~~~

XXX

Classes and instances
~~~~~~~~~~~~~~~~~~~~~

XXX

Termination
~~~~~~~~~~~

XXX termination + soundness + most-precise-fixpoint-ness + complexity 


Non-static aspects
~~~~~~~~~~~~~~~~~~

XXX specialization (tons of fun)

XXX executing more user program code (idem)

XXX constant propagation to remove bootstrap-only code

XXX termination even with non-static aspects


Code generation: rewriting to low-level operations
--------------------------------------------------

XXX introduction, repr

Low-level type system for C
~~~~~~~~~~~~~~~~~~~~~~~~~~~

XXX

Implementing operations as helpers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

XXX XXX reusing the annotator and specialization

Generating C code
~~~~~~~~~~~~~~~~~

XXX collecting functions and data structures recursively

XXX inserting hand-written C functions for suggested_primitives

XXX messy



.. _architecture: architecture.html
.. _`Thunk Object Space`: objspace.html#the-thunk-object-space
.. _`abstract interpretation`: theory.html#abstract-interpretation
.. _`Flow Object Space`: objspace.html#the-flow-object-space
.. _`Standard Object Space`: objspace.html#the-standard-object-space
.. _Psyco: http://psyco.sourceforge.net/
.. _`ACM SIGPLAN 2004 paper`: http://psyco.sourceforge.net/psyco-pepm-a.ps.gz
.. _`Hindley-Milner`: http://en.wikipedia.org/wiki/Hindley-Milner_type_inference

.. include:: _ref.txt
