==========================================================================================
Memory management and threading models as translation aspects -- solutions and challenges
==========================================================================================

.. contents::
.. sectnum::

Abstract
=========

One of the goals of the PyPy project is it to have the memory and threading
model flexible and changeable without having to touch the interpreter
everywhere.  This document describes the current state of the implementation of
memory object model, automatic memory management and threading models as well
as possible future developments.

XXX

Background
===========

The main emphasis of the PyPy project is that of integration: we want to make
changing memory management and threading techniques possible while at the same
time influencing the interpreter as little as possible. It is not the current
goal to optimize the current approaches in extreme ways rather to produce solid
implementations and to provide an environment where experiments with
fundamentally different ways to implement these things is possible and
reasonably easy. 

XXX

The low level object model
===========================

  - low level object model, data structures current layouts (also a word on the
    example of cached functions with PBC argument?)
  - how we deal with id hashes
  - probably describe in more detail the possibilies to completely change the
    representation of objects, etc.

XXX how detailed does this need to be described here? 

Automatic Memory Management Implementations
============================================

The whole implementation of the PyPy interpreter assumes automatic memory
management, e.g. automatic reclamation of memory that is no longer used. The
whole analysis toolchain also assumes that memory management is being taken
care of. Only the backends have to concern themselves with that issue. For
backends that target environments that have their own garbage collector, like
Smalltalk or Javascript, this is not an issue. For other targets like C and
LLVM the backend has to produce code that uses some sort of garbage collection.

This approach has several advantages. It makes it possible to target different
platforms, with and without integrated garbage collection. Furthermore the
interpreter implementation is not complicated by the need to do explicit memory
management everywhere. Even more important the backend can optimize the memory
handling to fit a certain situation (like a machine with very restricted
memory) or completely replace the memory management technique or memory model
with a different one without having to change interpreter code. Additionally
the backend can use information that was inferred by the rest of the toolchain
to improve the quality of memory management. 

Using the Boehm garbage collector
-----------------------------------

At the moment there are two different garbage collectors implemented in the C
backend (which is the most complete backend right now). One of them uses the
existing Boehm-Demers-Weiser garbage collector [BOEHM]_. For every memory
allocating operation in a low level flow graph the C backend introduces a call
to a function of the boehm collector which returns a suitable amount of memory.
Since the C backends has a lot of information avaiable about the data structure
being allocated it can choose the memory allocation function out of the Boehm
API that fits best. For example for objects that do not contain references to
other objects (e.g. strings) there is a special allocation function that
signals to the collector that it does not need to consider this memory when
tracing pointers.

Using the Boehm collector has disadvantages as well. Its problems stem from the
fact that the Boehm collector is conservative which means that it has to
consider every word in memory to be a potential pointer. Since PyPy's toolchain
has complete knowledge of the placement of data in memory we can generate an
exact garbage collector that considers only pointers.

Using a simple reference counting garbage collector
-----------------------------------------------------

The other implemented garbage collector is a simple reference counting scheme.
The C backend inserts a reference count field into every structure that has to be
handled by the garbage collector and puts increment and decrement operations
for this reference count into suitable places in the resulting C code. After
every reference decrement operations a check is performed whether the reference
count has dropped to zero. If this is the case the memory of the object will be
reclaimed after the references counts of the objects the original object
references are decremented as well.

The current placement of reference counter updates is far from optimal: The
reference counts are updated much more often than theoretically necessary (e.g.
sometimes a counter is increased and then immediately decreased again).
Furthermore some more analysis could show that some objects don't need a
reference counter at all because they either have a very short, foreseeable
life-time or because they live exactly as long as another object.

Another drawback of the current reference counting implementation is that it
cannot deal with circular references, which is a fundamental flaw of reference
counting memory management schemes in general. CPython solves this problem by
having special code that handles circular garbage which PyPy lacks at the
moment. This problem has to be addressed in the future to make the reference
counting scheme a viable garbage collector. Since reference counting is quite
succesfully used by CPython it will be interesting to see how far it can be
optimized for PyPy.

Simple escape analysis to remove memory allocation
---------------------------------------------------

We also implemented a technique to prevent some amount of memory allocation.
Sometimes it is possible to deduce from the flow graphs that an object lives
exactly as long as the stack frame of the function where it is allocated in.
This happens if no pointer to the object is stored into another object and if
no pointer to the object is returned from the function. If this is the case and
if the size of the object is known in advance the object can be allocated on
the stack. To achieve this, the object is "exploded", that means that for every
element of the structure a new variable is generated that is handed around in
the graph. Reads from elements of the structure are removed and just replaced
by one of the variables, writes by assignements to same.

Since quite a lot of objects are allocated in small "helper" functions this
simple approach which does not track objects accros function boundaries only
works well in the presence of function inlining.

XXX

A general garbage collection framework
--------------------------------------

(XXX I have no idea whether/how detailed this should be described here. It kind
of fits the "solutions for memory models", though)

In addition to the garbage collectors implemented in the C backend we have also
started writing a more general toolkit for implementing exact garbage
collectors in Python. The general idea is to express the garbage collection
algorithms in Python as well and translate them as part of the translation
process to C code (or whatever the intended platform is).

To be able to access memory in a low level manner there are special ``Address``
objects that behave like pointers to memory and can be manipulated accordingly:
it is possible to read/write to the location they point to a variety of data
types and to do pointer arithmetic.  These objects are translated to real
pointers and the appropriate operations. When run on top of CPython there is a
*memory simulator* that makes the address objects behave like they were
accessing real memory. In addition the memory simulator contains a number of
consistency checks that expose common memory handling errors like dangling
pointers, uninitialized memory, etc.

At the moment we have three simple garbage collectors implemented for this
framework: a simple copying collector, a mark-and-sweep collector and a
deferred reference counting collector. These garbage collectors are working on
top of the memory simulator at the moment it is not yet possible to translate
PyPy to C with them, though. This is due to the fact that it is not easy to
find the root pointers that reside on the C stack because the C stack layout is
heavily platform dependent and because of the possibility of roots that are not
only on the stack but also in registers (which would give a problem for moving
garbage collectors).

There are several possible solutions for this problem: One
of them is to not use C compilers to generate machine code so that the stack
frame layout can be controlled by us. This is one of the tasks that need to be
tackled in phase 2 as generating assembly directly is needed anyway for a
just-in-time compiler. The other possibility (which would be much easier to
implement) is to move all the data away from the stack to the heap, as
described below in section XXXXX.

Threading Model Implementations
============================================

XXX nice introductory paragraph

No threading
-------------

By default multi-threading is not supported at all, which gives some small
benefits for single-threaded applications since even for single-threaded
applications some overhead is there if threading-capabilities are built into
the interpreter.

Threading with a Global Interpreter Lock
------------------------------------------

At the moment there is one non-trivial threading model implemented. It follows
the threading implementation of CPython and thus uses a global interpreter
lock. This lock prevents that any two threads can interpret python code at any
time. The global interpreter lock is released around calls to blocking I/O functions. 
This approach has a number of advantages: it gives very little runtime penalty
for single-threaded applications, makes many of the common uses for threading
possible and is relatively easy to implement and maintain. It has the
disadvantages that multiple threads cannot be distributed accros multiple
proccessors (XXX is this really a major point? it is repeated very often which
of course does not make it true).

XXX GIL release around system calls (how do we mention that here? as if it was
already implemented?)

Stackless C code
-----------------

"Stackless" C code is C code that only uses a bounded amount of the
space in the C stack, and that can more generally obtain explicit
control of its own stack.  This is generally known as "continuations",
or "continuation-passing style" code, although in our case we will limit
ourselves to single-shot continuations, i.e. continuations that are
captured and subsequently only resumed once.

The technique we have implemented is based on an old but recurring idea
of emulating this style via exceptions: a specific program point can
generate a pseudo-exception whose purpose is to unwind the whole C stack
in a restartable way.  More precisely, the "unwind" exception has the
effect of saving the C stack into the heap, in a compact and explicit
format, as described below.  It is then possible to resume only the
innermost (most recent) frame of the saved stack -- allowing unlimited
recursion on OSes that limit the size of the C stack -- or to resume a
different previously-saved C stack altogether, thus implementing
coroutines.

In our case, exception handling is always explicit: the C backend always
puts after each call site a cheap check to detect if the callee exited
normally or generated an exception.  So when compiling functions in
stackless mode, the generated exception handling code special-cases the
new "unwind" exception.  This exception causes the current function to
respond by saving its local variables to a heap structure (a linked list
of records, one per stack frame) and then propagating the exception
outwards.  Eventually, at the end of the frame chain, the outermost
function is a manually-written dispatcher that catches the "unwind"
exception.

At this point, the whole C stack is stored away in the heap.  This is a
very interesting state in itself, because precisely there is no C stack
left.  It allows us to write in a portable way all the algorithms that
normally require machine-specific instructions to inspect the stack,
e.g. garbage collectors.

To continue execution, the dispatcher can resume either the freshly
saved or a completely different stack.  Moreover, it can resume directly
the innermost (most recent) saved frame in the heap chain, without
having to resume all intermediate frames first.  This not only makes
stack switches fast, but it also allows the frame to continue to run on
top of a clean C stack.  When that frame eventually exits normally, it
returns to the dispatcher, which then invokes the previous (parent)
saved frame, and so on.  In this model, the C stack can be considered as
a cache for the heap-based saved frame.  When we run out of C stack
space, we flush the cache.  When the cache is empty, we fill it with the
next item from the heap.

To give the translated program some amount of control over the
heap-based stack structures and over the top-level dispatcher that jumps
between them, there are a few "external" functions directly implemented
in C.  These functions provide an elementary interface on top of which
useful abstractions can be implemented, like:

* coroutines: explicitly switching code, similar to Greenlets_.

* "tasklets": cooperatively-scheduled microthreads, as introduced in
  `Stackless Python`_.

* implicitly-scheduled microthreads, also known as green threads.

An important property of the changes in all the generated C functions is
to be written in a way that almost does not degrade their performance in
the non-exceptional case.  Most optimisations performed by C compilers,
like register allocation, continue to work...

.. image:: image/stackless_informal.png


Open Challenges
================

XXX

open challenges for phase 2:

  - more clever incref/decref policy, circularity detector
  - more sophisticated structure inlining ?  possibly
  - full GC hooks? (we have started a framework for GC construction, only simulated for now)
  - exact GC needs -- control over low-level machine code
  - Finalization and weak references
  - green threads? 
  - threading model with various granularities of locking


Conclusion
===========

XXX nice concluding paragraphs


References
===========

.. [BOEHM] `Boehm-Demers-Weiser garbage collector`_, a garbage collector
           for C and C++, Hans Boehm, 1988-2004
.. _`Boehm-Demers-Weiser garbage collector`: http://www.hpl.hp.com/personal/Hans_Boehm/gc/

.. _Greenlets: http://codespeak.net/py/current/doc/greenlet.html
.. _`Stackless Python`: http://www.stackless.com
