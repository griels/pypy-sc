========================================================================
                        JIT Generation in PyPy
========================================================================

.. contents::
.. sectnum::


------------------------------------------------------------------------
                           Usage and Status
------------------------------------------------------------------------

Status
======

A foreword of warning about the JIT of PyPy as of March 2007: single
functions doing integer arithmetic get great speed-ups; about anything
else will be a bit slower with the JIT than without.  We are working
on this - you can even expect quick progress, because it is mostly a
matter of adding a few careful hints in the source code of the Python
interpreter of PyPy.

By construction, the JIT is supposed to work correctly on absolutely any
kind of Python code: generators, nested scopes, ``exec`` statements,
``sys._getframe().f_back.f_back.f_locals``, etc. (the latter is an
example of expression that Psyco_ cannot emulate correctly).  However,
there are a couple of known issues for now (see Caveats_).


In more details
---------------

So far there is little point in trying the JIT on anything else than
arithmetic-intensive functions (unless you want to help find bugs).  For
small examples, you can also look at the machine code it produces, but
if you do please keep in mind that the assembler will look fundamentally
different after we extend the range of PyPy that the JIT generator
processes.

* The produced machine code is kind of good, in the sense that the
  backends perform some reasonable register allocation.  The 386 backend
  computes the lifetime of values within blocks.  The PPC backend does
  not, but the fact that the processor has plenty of registers mitigates
  this problem to some extent.  The PPC backend has at least one known
  bug left.  An LLVM_ backend is started but blocked half-way on a hard
  problem that might not get solved any time soon.

* The *timeshifter*, which produces the JIT frontend, is able to handle
  rather incredibly tricky situations successfully.

* The remaining work is to continue the necessary adjustments of the
  PyPy interpreter source code so that the timeshifter can process more
  of it.  At the moment, the timeshifter sees the interpreter's main
  dispatch loop, integer arithmetic, and a bit of function call logic.
  This means that the produced JIT can remove the bytecode
  interpretation overhead and do a good job with integer arithmetic, but
  cannot optimize at all the manipulations of any other type of objects.

.. _LLVM: http://llvm.org/


How to compile a pypy-c with a JIT
==================================

Go to ``pypy/translator/goal/`` and run::

    ./translate.py --jit targetpypystandalone

Please read the Status_ section above first.

This will produce the C code for a version pypy-c that includes both a
regular interpreter and an automatically generated JIT compiler.  This
pypy-c uses its interpreter by default, and due to some overhead we
expect this interpreter to be a bit slower than the one found in a
pypy-c compiled without JIT.

In addition to ``--jit``, you can also pass the normal options to
``translate.py`` to compile different flavors of PyPy with a JIT.  See
the `compatibility matrix`_ for the combinations known to be working
right now.  (The combination of the JIT with the thunk or taint object
spaces probably works too, but we don't expect it to generate good code
before we drop a few extra hints in the source code of the object
spaces.)

.. _`compatibility matrix`: image/compat-matrix.png

Usage
=====

You can mark one or many code objects as candidates for being run by
the JIT as follows::

    >>>> def f(x): return x*5
    >>>> import pypyjit
    >>>> pypyjit.enable(f.func_code)
    >>>> f(7)
    # the JIT runs here
    35
    >>>> f(8)
    # machine code already generated, no more jitting occurs here
    40

A few examples of this kind can be found in `demo/jit/`_.  The script
`demo/jit/f1.py`_ shows a function that becomes seriously faster with
the JIT - only 10% to 20% slower than what ``gcc -O0`` produces from the
obvious equivalent C code, a result similar to Psyco.  Although the JIT
generation process is well-tested, we only have a few tests directly for
the final ``pypy-c``.  Try::

    pypy-c test_all.py module/pypyjit/test/test_pypy_c.py -A --nomagic

You can get a dump of the generated machine code by setting the
environment variable ``PYPYJITLOG`` to a file name before you start
pypy-c.  See `In more details`_ above.  To inspect this file, use the
following tool::

    python  pypy/jit/codegen/i386/viewcode.py  dumpfilename

The viewcode.py script is based on the Linux tool ``objdump`` to produce
a disassembly.  It should be easy to port to OS/X.  If you want to port
the tool to Windows, have a look at
http://codespeak.net/svn/psyco/dist/py-utils/xam.py : this is the tool
from which viewcode.py was derived in the first place, but
Windows-specific parts were omitted for lack of a Windows machine to try
them on.

Caveats
-------

When running JIT'ed code, the bytecode tracing hook is not invoked.  This
should be the only visible effect of the JIT, aside from the debug
prints and the speed/memory impact.  In practice, of course, it still
has also got rough edges.

One of them is that all compile-time errors are fatal for now, because
it is hard to recover from them.  Clearly, the compiler is not
*supposed* to fail, but it can occur because the memory runs out, a bug
hits, or for more subtle reasons.  For example, overflowing the stack is
likely to cause the JIT compiler to try to compile the app-level
handlers for the RuntimeError, and compiling takes up stack space too -
so the compiler, running on top of the already-full stack, might hit the
stack limit again before it has got a chance to generate code for the
app-level handlers.


------------------------------------------------------------------------
                     Make your own JIT compiler
------------------------------------------------------------------------

Introduction
============

The central idea of the PyPy JIT is to be *completely independent from
the Python language*.  We did not write down a JIT compiler by hand.
Instead, we generate it anew during each translation of pypy-c.

This means that the same technique works out of the box for any other
language for which we have an interpreter written in RPython.  The
technique works on interpreters of any size, from tiny to PyPy.

Aside from the obvious advantage, it means that we can show all the
basic ideas of the technique on a tiny interpreter.  The fact that we
have done the same on the whole of PyPy shows that the approach scales
well.  So we will follow in the sequel the example of a tiny interpreter
and insert a JIT compiler into it during translation.

A tiny interpreter
==================




------------------------------------------------------------------------
                  JIT Compiler Generation - Theory
------------------------------------------------------------------------

Introduction
============

One the of the central goals of the PyPy project is to automatically
produce a Just in Time Compiler from the interpreter, with as little
as possible intervention on the interpreter codebase itself.  The Just
in Time Compiler should be another aspect as much as possible
transparently introduced by and during the translation process.

Partial evaluation techniques should, at least theoretically, allow
such a derivation of a compiler from an interpreter. (XXX references)

The forest of flow graphs that the translation process generates and
transforms constitutes a reasonable base for the necessary analyses.
That's a further reason why having an high-level runnable and
analysable description of the language was always a central tenet of
the project.

Transforming an interpreter into a compiler involves constructing a so
called *generating extension*, which takes input programs to the
interpreter, and produces what would be the output of partially
evaluating the interpreter, with the input program fixed and the input
data left as a variable. The generating extension is essentially
capable of compiling the input programs.

Generating extensions can be produced by self-applying partial evaluators,
but this approach may lead to not optimal results or be not scalable
(XXX expand this argument).

For PyPy, our approach aims at producing the generating extension more
directly from the analysed interpreter in the form of a forest of flow
graphs. We call such process *timeshifting*.

To be able to achieve this, gathering *binding time* information is
crucial.  This means distinguishing values in the data-flow of the
interpreter which are compile-time bound and immutable at run-time,
versus respectively runtime values.

Currently we base the binding time computation on propagating the
information based on a few hint inserted in the interpreter. Propagation
is implemented by reusing our `annotation/type inference framework`__.

__ annotator_

The code produced by a generating extension for an input program may
not be good, especially for a dynamic language, because essentially
the input program doesn't contain enough information to generate good
code. What is really desired is not a generating extension doing
static compilation, but one capable of dynamic compilation, exploiting
runtime information in its result. Compilation should be able to
suspend, let the produced code run to collect run-time information
(for example language-level types), and then resume with this extra
information.  This allow it to generate code optimised for the effective
run-time behaviour of the program.

Inspired by Psyco, which is in some sense a hand-written generating
extension for Python, we added support
for so-called *promotion* to our framework for producing generating
extensions.

Simply put, promotion on a value stops compilation and waits until the
runtime reaches this point.  When it does, the actual runtime value is
promoted into a compile-time constant, and compilation resumes with this
extra information.  Concretely, the promotion expands into a switch in
the generated code.  The switch contains one case for each runtime value
encountered so far, to chose which of the specialized code paths the
runtime execution should follow.  The switch initially contains no case
at all, but only a fall-back path.  The fall-back invokes the compiler
with the newly seen runtime value.  The compiler produces the new
specialized code path.  It then patches the switch to add this new case,
so that the next time the same value is encountered at runtime, the
execution can directly jump to the correct specialized code path.

This can also be thought of as a generalisation of polymorphic inline
caches (XXX reference).


Partial Evaluation
==================

Partial evaluation is the process of evaluating a function, say ``f(x,
y)``, with only partial information about the value of its arguments,
say the value of the ``x`` argument only.  This produces a *residual*
function ``g(y)``, which takes less arguments than the original - only
the information not specified during the partial evaluation process need
to be provided to the residual function, in this example the ``y``
argument.

Partial evaluation (PE) comes in two flavors:

* *On-line* PE: a compiler-like algorithm takes the source code of the
  function ``f(x, y)`` (or its intermediate representation, i.e. its
  control flow graph in PyPy's terminology), and some partial
  information, e.g. ``x = 5``.  From this, it produces the residual
  function ``g(y)`` directly, by following in which operations the
  knowledge ``x = 5`` can be used, which loops can be unrolled, etc.

* *Off-line* PE: in many cases, the goal of partial evaluation is to
  improve performance in a specific application.  Assume that we have a
  single known function ``f(x, y)`` in which we think that the value of
  ``x`` will change slowly during the execution of our program - much
  more slowly than the value of ``y``.  An obvious example is a loop
  that calls ``f(x, y)`` many times with always the same value ``x``.
  We could then use an on-line partial evaluator to produce a ``g(y)``
  for each new value of ``x``.  In practice, the overhead of the partial
  evaluator might be too large for it to be executed at run-time.
  However, if we know the function ``f`` in advance, and if we know
  *which* arguments are the ones that we will want to partially evaluate
  ``f`` with, then we do not need a full compiler-like analysis of ``f``
  every time the value of ``x`` changes.  We can precompute off-line a
  specialized function ``f1(x)``, which when called produces a residual
  function ``g(y)``.

Off-line partial evaluation is based on *binding-time analysis*, which
is the process of determining among the variables used in a function (or
a set of functions) which ones are going to be known in advance and which
ones are not.  In the above example, such an analysis would be able to
infer that the constantness of the argument ``x`` implies the
constantness of many intermediate values used in the function.  The
*binding time* of a variable determines how early the value of the
variable will be known.

The PyPy JIT is generated using off-line partial evaluation.  As such,
there are three distinct phases:

* *Translation time*: during the normal translation of an RPython
  program like PyPy, we perform binding-time analysis and off-line
  specialization.  This produces a new set of functions (``f1(x)`` in
  our running example) which are linked with the rest of the program.

* *Compile time*: during the execution of the program, when a new value
  for ``x`` is found, ``f1(x)`` is invoked.  All the computations
  performed by ``f1(x)`` are called compile-time computations.  This is
  justified by the fact that ``f1(x)`` is in some sense a compiler,
  whose sole effect is to produce residual code.

* *Run time*: the normal execution of the program.

The binding-time terminology that we are using in PyPy is based on the
colors that we use when displaying the control flow graphs:

* *Green* variables contain values that are known at compile-time -
  e.g. ``x``.

* *Red* variables contain values that are not known until run-time -
  e.g. ``y``.


For more information
====================

The `expanded version of the present document`_ is mostly unreadable,
but may be of interest to you if you are already familiar with the
domain of Partial Evaluation.

.. _`expanded version of the present document`: discussion/jit-draft.html


.. _VMC: http://codespeak.net/svn/pypy/extradoc/talk/dls2006/pypy-vm-construction.pdf
.. _`RPython`: coding-guide.html#rpython
.. _`RPython Typer`: translation.html#rpython-typer
.. _`low-level graphs`: rtyper.html
.. _`pointer-and-structures-like objects`: rtyper.html#low-level-types 
.. _`annotator`: dynamic-language-translation.html
.. _`specialization of functions`: dynamic-language-translation.html#specialization
.. _Psyco: http://psyco.sourceforge.net
.. _`PyPy Standard Interpreter`: architecture.html#standard-interpreter
.. _`exception transformer`: translation.html#making-exception-handling-explicit

.. include:: _ref.txt
