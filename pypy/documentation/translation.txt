=====================
    Translation
=====================

.. contents::
.. sectnum::

This document describes the tool chain that we developed to analyze and
"compile" RPython_ programs (like PyPy itself) to various lower-level
languages.

.. _RPython: coding-style.html#restricted-python


Overview
========

XXX very preliminary documentation!

The module `translator.py`_ is the common entry point to the various parts
of the translation process.  It is available as an interactive utility to
`play around`_.

Here are the steps we follow to translate a given program:

1. The complete program is imported.  If needed, extra initialization is performed.  Once this is done, the program must be present in memory is a form that is "static enough" in the sense of RPython_.

2. The `Flow Object Space`_ processes the input program, turning each function independently into a `control flow graph`_ data structure recording sequences of basic operations in "single-style assignment".

3. Optionally, the Annotator_ performs global type inference on the control flow graphs.  Each variable gets annotated with an inferred type.

4. One of the Code Generators (XXX not documented yet) turns the optionally annotated flow graphs and produces a source file in a lower-level language: C_, LLVM_, `Common Lisp`_, Pyrex_, Java_, or `Python again`_ (this is used in PyPy to turn sufficiently RPythonic app-level code into interp-level code).

5. This lower-level source file is compiled to produce an executable.

.. _`translator.py`: http://codespeak.net/svn/pypy/dist/pypy/translator/translator.py
.. _`play around`: getting_started.html#trying-out-the-translator
.. _`Flow Object Space`: objspace.html#the-flow-object-space
.. _`control flow graph`: objspace.html#the-flow-model
.. _C: http://codespeak.net/svn/pypy/dist/pypy/translator/genc/
.. _LLVM: http://codespeak.net/svn/pypy/dist/pypy/translator/llvm/
.. _`Common Lisp`: http://codespeak.net/svn/pypy/dist/pypy/translator/gencl.py
.. _Pyrex: http://codespeak.net/svn/pypy/dist/pypy/translator/genpyrex.py
.. _Java: http://codespeak.net/svn/pypy/dist/pypy/translator/java/
.. _`Python again`: http://codespeak.net/svn/pypy/dist/pypy/translator/geninterplevel.py



.. _Annotator:

The annotation pass
===================

(INCOMPLETE DRAFT)

We describe below how a control flow graph can be "annotated" to 
discover the types of the objects.  This annotation pass is a form of 
type inference.  It is done after control flow graphs are built by the 
FlowObjSpace, but before these graphs are translated into low-level code 
(e.g. C/Lisp/Pyrex).


Model
------------------------

The major goal of the annotator is to "annotate" each variable that 
appears in a flow graph.  An "annotation" describes all the possible 
Python objects that this variable could contain at run-time, based on a 
whole-program analysis of all the flow graphs --- one per function.

An "annotation" is an instance of ``SomeObject``.  There are subclasses 
that are meant to represent specific families of objects.  Note that 
these classes are all meant to be instantiated; the classes ``SomeXxx`` 
themselves are not the annotations.

Here is an overview (see ``pypy.annotation.model``):

* ``SomeObject`` is the base class.  An instance ``SomeObject()`` 
  represents any Python object.  It is used for the case where we don't 
  have enough information to be more precise.  In practice, the presence 
  of ``SomeObject()`` means that we have to make the annotated source code 
  simpler or the annotator smarter.

* ``SomeInteger()`` represents any integer.  
  ``SomeInteger(nonneg=True)`` represent a non-negative integer (``>=0``).

* ``SomeString()`` represents any string; ``SomeChar()`` a string of 
  length 1.

* ``SomeTuple([s1,s2,..,sn])`` represents a tuple of length ``n``.  The 
  elements in this tuple are themselves constrained by the given list of 
  annotations.  For example, ``SomeTuple([SomeInteger(), SomeString()])`` 
  represents a tuple with two items: an integer and a string.

There are more complex subclasses of ``SomeObject`` that we describe in 
more details below.

All the ``SomeXxx`` instances can optionally have a ``const`` attribute, 
which means that we know exactly which Python object the Variable will 
contain.

All the ``SomeXxx`` instances are supposed to be immutable.  The 
annotator manages a dictionary mapping Variables (which appear in flow 
graphs) to ``SomeXxx`` instances; if it needs to revise its belief about 
what a Variable can contain, it does so by updating this dictionary, not 
the ``SomeXxx`` instance.


Annotator
--------------------------

The annotator itself (``pypy.translator.annrpython``) works by 
propagating the annotations forward in the flow graphs, starting at some 
entry point function, possibly with explicitely provided annotations 
about the entry point's input arguments.  It considers each operation in 
the flow graph in turn.  Each operation takes a few input arguments 
(Variables and Constants) and produce a single result (a Variable).  
Depending on the input argument's annotations, an annotation about the 
operation result is produced.  The exact rules to do this are provided 
by the whole ``pypy.annotation`` subdirectory, which defines all the 
cases in detail according to the R-Python semantics.  For example, if 
the operation is 'v3=add(v1,v2)' and the Variables v1 and v2 are 
annotated with ``SomeInteger()``, then v3 also receives the annotation 
``SomeInteger()``.  So for example the function::

    def f(n):
        return n+1

corresponds to the flow graph::

    start ----------.
                    |
                    V 
           +-------------------+
           |  v2 = add(v1, 1)  |
           +-------------------+
                    |
                    `---> return block

If the annotator is told that v1 is ``SomeInteger()``, then it will 
deduce that v2 (and hence the function's return value) is 
``SomeInteger()``.

This step-by-step annotation phase proceeds through all the operations 
in a block, and then along the links between the blocks of the flow 
graph.  If there are loops in the flow graph, then the links will close 
back to already-seen blocks, as in::

    def g(n):
        i = 0
        while n:
            i = i + n
            n = n - 1

whose flow graph is::

    start -----.           ,-----------------.
               | n1 0      | m3 j3           |
               V           v                 |
           +-------------------+             |
           |   input: n2 i2    |             |
           |  v2 = is_true(n2) |             |
           +-------------------+             |
               |             |               |
               |ifFalse      |ifTrue         |
    return <---'             | n2 i2         |
                             V               |
                    +--------------------+   |
                    |   input: n3 i3     |   |
                    |  j3 = add(i3, n3)  |   |
                    |  m3 = sub(n3, 1)   |---'
                    +--------------------+

Be sure to follow the variable renaming that occurs systematically 
across each link in a flow graph.  In the above example the Variables 
have been given names similar to the name of the original variables in 
the source code (the FlowObjSpace tries to do this too) but keep in mind 
that all Variables are different: n1, n2, i2, v2, n3, i3, j3, m3.

Assume that we call the annotator with an input annotation of 
``SomeInteger()`` for n1.  Following the links from the start, the 
annotator will first believe that the Variable i2, whose value comes 
from the constant 0 of the first link, must always be zero.  It will 
thus use the annotation ``SomeInteger(const=0)`` for i2.  Then it will 
propagate the annotations through both blocks, and find that v2 is 
``SomeBool()`` and all other variables are ``SomeInteger()``.  In 
particular, the annotation of j3 is different from the annotation of the 
Variable i2 into which it is copied (via the back-link).  More 
precisely, j3 is ``SomeInteger()`` but i2 is the more specific 
``SomeInteger(const=0)``.  This means that the assumption that i2 must 
always be zero is found to be wrong.  At this point, the annotation of 
i2 is *generalized* to include both the existing and the new annotation.  
(This is the purpose of ``pypy.annotation.model.unionof()``).  Then 
these more general annotations must again be propagated forward.

This process of successive generalizations continues until the 
annotations stabilize.  In the above example, it is sufficient to 
re-analyse the first block once, but in general it can take several 
iterations to reach a fixpoint.  Annotations may also be propagated from 
one flow graph to another and back repeatedly, across ``call`` 
operations.  The overall model should ensure that this process 
eventually terminates under reasonable conditions.  Note that as long as 
the process is not finished, the annotations given to the Variables are 
wrong, in the sense that they are too specific; at run-time, the 
Variables will possibly contain Python objects outside the set defined 
by the annotation, and the annotator doesn't know it yet.


Description of the available types
-----------------------------------------------

The reference and the details for the annotation model is found in the 
module ``pypy.annotation.model``.  We describe below the issues related 
to the various kinds of annotations.


Simple Types
++++++++++++

``SomeInteger``, ``SomeBool``, ``SomeString``, ``SomeChar`` all stands 
for the obvious corresponding set of immutable Python objects.


Tuples
++++++

``SomeTuple`` only considers tuples of known length.  We don't try to 
handle tuples of varying length (the program should use lists instead).


Lists and Dictionaries
++++++++++++++++++++++

``SomeList`` stands for a list of homogenous type (i.e. all the elements 
of the list are represented by a single common ``SomeXxx`` annotation).

``SomeDict`` stands for a homogenous dictionary (i.e. all keys have the 
same ``SomeXxx`` annotation, and so have all values).

These types are mutable, which requires special support for the 
annotator.  The problem is that in code like::

   lst = [42]
   update_list(lst)
   value = lst[0]

the annotation given to ``value`` depends on the order in which the 
annotator progresses.  As ``lst`` is originally considered as a list of 
``SomeInteger(const=42)``, it is possible that ``value`` becomes 
``SomeInteger(const=42)`` as well if the analysis of ``update_list()`` 
is not completed by the time the third operation is first considered.  
To solve this problem, each ``SomeList`` or ``SomeDict`` is linked to a 
set of so-called *factories*.  Each creation point, i.e. each 'newlist' 
or 'newdict' operation, gets its associated factory.  The factory 
remembers what kind of object it really needs to build.  For example, in 
code like::

   lst = [42]
   lst.append(43)

the factory associated with the first line originally builds a list 
whose items are all constants equal to 42; when the ``append(43)`` call 
is then found, the factory is updated to build a more general list of 
integers, and the annotator restarts its analysis from the factory 
position.  Our model is not sensitive to timing: it doesn't know that 
the same list object may contain different items at different times.  It 
only computes how general the items in the list must be to cover all 
cases.

For initially empty lists, as created by ``lst = []``, we build a list 
whose items have the annotation ``SomeImpossibleValue``.  This is an 
annotation that denotes that no Python object at all can possibly appear 
here at run-time.  It is the least general annotation.  The rationale is 
that::

   lst = []
   oups = lst[0]

will give the variable ``oups`` the annotation ``SomeImpossibleValue``, 
which is reasonable given that no concrete Python object can ever be put 
in ``oups`` at run-time.  In a more usual example::

   lst = []
   lst.append(42)

the list is first built with ``SomeImpossibleValue`` items, and then the 
factory is generalized to produce a list of ``SomeInteger(const=42)``.  
With this "impossible object" trick we don't have to do anything special 
about empty lists.


User-defined Classes and Instances
++++++++++++++++++++++++++++++++++

``SomeInstance`` stands for an instance of the given class or any 
subclass of it.  For each user-defined class seen by the annotator, we 
maintain a ClassDef (``pypy.annotation.classdef``) describing the 
attributes of the instances of the class; essentially, a ClassDef gives 
the set of all class-level and instance-level attributes, and for each 
one, a corresponding ``SomeXxx`` annotation.

Instance-level attributes are discovered progressively as the annotation 
progresses.  Assignments like::

   inst.attr = value

update the ClassDef of the given instance to record that the given 
attribute exists and can be as general as the given value.

For every attribute, the ClassDef also records all the positions where 
the attribute is *read*.  If, at some later time, we discover an 
assignment that forces the annotation about the attribute to be 
generalized, then all the places that read the attribute so far are 
marked as invalid and the annotator will have to restart its analysis 
from there.

The distinction between instance-level and class-level attributes is 
thin; class-level attributes are essentially considered as initial 
values for instance-level attributes.  Methods are not special in this 
respect, expect that they are bound to the instance (i.e. ``self = 
SomeInstance(cls)``) when considered as the initial value for the 
instance.

The inheritance rules are as follows: the union of two ``SomeInstance`` 
annotations is the ``SomeInstance`` of the most precise common base 
class.  If an attribute is considered (i.e. read or written) through a 
``SomeInstance`` of a parent class, then we assume that all subclasses 
also have the same attribute, and that the same annotation applies to 
them all (so code like ``return self.x`` in a method of a parent class 
forces the parent class and all its subclasses to have an attribute 
``x``, whose annotation is general enough to contain all the values that 
all the subclasses might want to store in ``x``).  However, distinct 
subclasses can have attributes of the same names with different, 
unrelated annotations if they are not used in a general way through the 
parent class.


Prebuilt Constants
++++++++++++++++++

(to be completed)


Built-in functions and methods
++++++++++++++++++++++++++++++

(to be completed)


Others
++++++

(to be completed)




The C Back-End
==============


Overview
--------

The task of GenC is to convert a flow graph into C code.  By itself, GenC does not use the annotations in the graph.  It can actually convert unannotated graphs to C.  However, to make use of the annotations if they are present, an extra pass is needed: the C typer, whose task is to modify the flow graph according to the annotations, replacing operations with lower-level C-ish equivalents.

The C typer first replaces all Variables that have constant annotations with real Constants in the flow graph.

For the time being, we assume that each SomeXxx annotation has a canonical C-level representation.  For example, all variables annotated with SomeInteger() will correspond to the C ``int`` type.  This allows an attribute ``concretetype`` to be attached to each Variable.  Constants also get such an attribute ``concretetype``, but they are computed based on the usage that is made of the particular Constant; for example, a Constant(5) will be a C ``int`` when used in an integer addition but a C ``PyObject*`` when inserted into a CPython list.

The C typer considers each operation in turn and, based on its annotations, replaces it by C-level operations if possible.


Example: Integer operations
---------------------------

Integer operations are the easiest.  Assume a graph containing the following operation::

    v3 = add(v1, v2)

annotated::

    v1 -> SomeInteger()
    v2 -> SomeInteger()
    v3 -> SomeInteger()

then obviously we want to type it and replace it with::

    int v1, v2, v3;
    v3 = int_add(v1, v2)

The typing notation in C syntax means that the Variables (which might actually be Constants) are given the ``concretetype`` corresponding to the C ``int``.


Memory model
------------

For more complicated examples we need to choose a memory model.  We will assume a simple model: local Variables store primitive values only (integers, pointers), and the heap stores structs.

We support this model using the following new graph operations (the result Variable is omitted when meaningless)::

    v2 = malloc()
    v2 = malloc(v1)
    incref(v3)
    decref(v3)

v2=malloc() allocates some bytes of heap memory, zero it, and returns a pointer to it.  The number of bytes is suitable for the type of v2.  For now, there is always a reference counter in a hidden header, initialized to 1.

incref() and decref() allow each value to be tracked.  The decref(v3) operation is used when the content of the value ``v3`` is about to be deleted.  GenC inserts an operation decref(v3) whenever the value in a local variable v3 is about to be forgotten (it isn't passed to the next basic block).  In addition, incref(v3) is inserted when a local Variable is about to be duplicated (it is passed several times to the next basic block).

The exact operations performed by incref(v3) and decref(v3) depend on the type of v3.  For pointer types, they increment and decrement the reference counter of whatever they point to.  For integers, they have no effect.

When decref() is applied on a pointer and the reference counter drops to zero, the destructor of the appropriate type is called; its action depends on the type, but (as in CPython) what it typically does is call decref() on each member of the structure and then release the occupied memory.

Note that for now malloc() returns memory initialized to zero.  This allows our destructors to cope with partially initialized data structures, should an error occur while they are constructed.

In C, it is common to see a struct definition whose last member is a variable-sized array.  For this case, v2=malloc(v1) allocates an amount of memory which is enough to store the fixed part of the struct plus ``v1`` items in the variable-sized part.


CType
-----

Each C type is represented by an instance of a concrete subclass of CType_.  Each instance has the following interface:

* a ``typename`` attribute: the name given to this type in C.

* nameof(obj): method returning a C expression representing the constant ``obj`` in this C type.

* init_globals(): called the first time genc sees this particular CType instance.  It generates C code that declares the type.

* collect_globals(): called from time to time -- more precisely, before each function body is written to the C file.  It can generate more C code related to this type, if needed (e.g. declaration of static variables of this type).


.. _CType: http://codespeak.net/svn/pypy/dist/pypy/translator/genc/basetype.py


C-level operations
------------------

Here are the new operations that the C typer introduces in flow graphs.  Ideally, a completely typed graph should only contain these operations, and no longer any of the original operations.

Integer arithmetic operations (see complete list in `ctyper.py`_)::

    v3 = int_add(v1, v2)              # v3 = v1+v2;   // int v1, v2, v3
    v3 = int_sub(v1, v2)              # v3 = v1-v2;   // int v1, v2, v3
    v3 = int_mul(v1, v2)              # v3 = v1*v2;   // int v1, v2, v3
    v3 = int_is_true(v1)              # v3 = v1?1:0;  // int v1, v3
    etc.

Function call::

    v3 = direct_call(v1, v2, ...)     # v3 = v1(v2, ...);

Memory management (see `Memory model`_)::

    v2 = malloc()                     # for T* v2, alloc and zero sizeof(T) bytes of memory
    v2 = malloc(v1)                   # same, but allocates sizeof(T) + v1*sizeof(T_item)
    incref(v3)                        # for T* v3, details depend on T
    decref(v3)                        # for T* v3, details depend on T

Pointer operations::

    v3 = arrow(v1, 'fieldname')       # v3 = v1->fieldname;   // if fieldname is a primitive
    v3 = substruct(v1, 'fieldname')   # v3 = &v1->fieldname;  // if fieldname is a struct
    v3 = subarray(v1, 'fieldname')    # v3 = v1->fieldname;   // if fieldname is an array
    arrow_set(v1, 'fieldname', v2)    # v1->fieldname = v2;
    v3 = ptr_add(v1, v2)              # v3 = v1+v2;    // T* v1, int v2, T* v3
    v3 = ptr_cast(v1)                 # v3 = (T*) v1;  // S* v1, T* v3

For regularity, all pointers should point to struct types.  Casting should follow the C99 aliasing rules: in our case, we should only use it when we have got a pointer ``S* v1``, and we discover (by inspecting what v1 points to) that it is actually just the first field of a larger structure of type T.

If you are confused by the three variants of the ``->`` operator, note that only arrow() reads the content of a field.  The substruct() operation returns a pointer to a substructure of the parent structure.  The subarray() operation does the same for arrays in the parent structure -- which is essentially the same as substruct(), and not as arrow().  Blame C syntax.


.. _`ctyper.py`: http://codespeak.net/svn/pypy/dist/pypy/translator/genc/ctyper.py




The LLVM Back-End
=================

Overview
--------

XXX preliminary notes only

The task of GenLLVM is to convert a flow graph into `LLVM code`_, which can
then be optimized and compiled by LLVM. GenLLVM depends heavily on the
annotations, functions without annotations cannot be translated. The flowgraph
is not changed by GenLLVM in contrast to GenC. After the generation and
compilation of the LLVM code a wrapper is generated (at the moment with the
help of Pyrex) wich wraps the arguments and return value of the entry
function. Thus it is possible to call the entry function from Python.

GenLLVM does not depend on the CPython runtime which has the drawback that most
functions with SomeObject annotations cannot be compiled properly -- the only
operations that are allowed on variables with SomeObject annotations are
isinstance and type.

GenLLVM creates for every object in the flow graph (e.g. constants, variables,
blocks...) an LLVM 'representation'. This representation knows how to
represent the corresponding object in LLVM and knows what code to generate for
space operations on the object, what global definitions the object needs etc.

Some examples to make this cleare: A `ClassRepr`_ object represents a class, a
`FuncRepr`_ object represent a function (or method). The following happens if
the space operation ``simple_call`` is performed in a flow grap: An
appropriate ``FuncRepr`` object is constructed which generates LLVM code for
the function it represents. Then the ``FuncRepr`` inserts the appropriate LLVM
instructions into the LLVM code of the function it is called from (sometime
this is more than just a call: the arguments have to be casted,
etc). Something similar happens if a class is instantiated: A ``ClassRepr`` is
created which generates LLVM that allocates enough memory for an instance of
the class and then (if the class or a subclass has an ``__init__`` method)
tells the ``FuncRepr`` of the appropriate ``__init__`` method to generate the
code for the call to it.

Every representation object has a some other representations it depends on: A
``ListRepr`` of lists instances of a class depends on the ``ClassRepr`` of
that class. This is to ensure that the typedef for that is written after the
typedef of the class. To ensure this the dependency tree of representations
traversed depth first when the LLVM code is written to a file.

.. _`LLVM code`: http://www.llvm.org
.. _`ClassRepr`: http://codespeak.net/svn/pypy/dist/pypy/translator/llvm/classrepr.py
.. _`FuncRepr`: http://codespeak.net/svn/pypy/dist/pypy/translator/llvm/funcrepr.py


Details about the representations
---------------------------------

Simple representations
++++++++++++++++++++++

There are some objects that have direct counterparts in LLVM: ints, floats,
chars (strings of length 1). Most space operations involving those are
implemented as `tiny function`_ (LLVM doesn't support macros since LLVM's .ll
files correspond directly to its bytecode format so that round trip
conversions are nearly lossless).


Function representation
+++++++++++++++++++++++

The representation of function in LLVM code is relatively easy since LLVM as
well as flow graph use SSA form. Furthermore LLVM supports exactly the kind of
control structures that the flow graphs feature: A function consists of basic
blocks that end with links to other blocks, data flows along these links. The
data flow is handled in LLVM by phi nodes: at the beginning of every block phi
nodes may be inserted. It determines the value of a variable depending on
which block branched to the currect block. Example::

    block1:
        %b = phi int [1, %block0], [2, %block2]

Here %b is 1 if control came from block0 and 2 if control came from block2.


List representation
+++++++++++++++++++

Lists are represented as arrays. The code for the basic operation on lists
(``getitem``, ``setitem``, ``add``, ``mul``, ``append``, ``pop``...) is
`written in C`_. This C code is then compiled to LLVM code with the help of
the LLVM C-front-end. The resulting LLVM code is then transformed (with search
and replace) to fit in with the rest of GenLLVM. To support lists with
different types of items the C code implements lists as arrays of pointers to
``item``, where ``item`` is a dummy struct that is replaced with whatever type
is wanted.


XXX More to come.



.. _`tiny function`: http://codespeak.net/svn/pypy/dist/pypy/translator/llvm/operations.ll
.. _`written in C`: http://codespeak.net/svn/pypy/dist/pypy/translator/llvm/list.c

